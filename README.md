# **CLIP Adaptation with PEFT Techniques for Base-to-Novel Classification**

## **Overview**

This project focuses on implementing **Parameter-Efficient Fine-Tuning (PEFT)** techniques to adapt the **CLIP** model \[1] for **base-to-novel classification** using the **Flowers102** dataset.
The main objective is to explore methods that improve CLIPâ€™s performance under few-shot conditions, while maintaining efficiency in terms of trainable parameters and computational cost.

---

## **Project Goals**

* Evaluate baseline **Zero-Shot CLIP** performance.
* Implement **prompt tuning** via **CoCoOp** \[2].
* Implement **LoRA fine-tuning** for CLIP \[3].
* Reproduce **DISEF** \[4], an improved variant of CLIP-LoRA that uses a synthetic data generation pipeline.
* Propose and evaluate **our improved DISEF approach** to enhance accuracy and reduce computational overhead.

---

## **Implemented Techniques**

1. **Zero-Shot CLIP**

   * Baseline evaluation using original CLIP without fine-tuning.

2. **CoCoOp**

   * Conditional prompt tuning with dynamic context tokens generated by a meta-network.

3. **CLIP-LoRA**

   * PEFT approach using **Low-Rank Adaptation (LoRA)** applied to attention layers (Wq, Wv) of CLIP encoders.

4. **DISEF**

   * Synthetic dataset generation to boost LoRA fine-tuning performance.

5. **Improved DISEF**

   * Our optimized strategy for DISEF to achieve better performance and reduce synthetic data generation costs.

---

## **Dataset**

* **Flowers102** dataset:

  * Base classes for training.
  * Novel classes for evaluation in few-shot scenarios.

---

## **Project Structure**

```
â”œâ”€â”€ notebooks/          # Jupyter notebooks for experiments
â”œâ”€â”€ src/                # Core implementation (models, training loops, utilities)
â”œâ”€â”€ configs/            # Configuration files for experiments
â”œâ”€â”€ results/            # Evaluation metrics and logs
â”œâ”€â”€ README.md           # Project documentation
```

---

## **Installation**

```bash
# Clone the repository
git clone https://github.com/your-username/clip-peft-adaptation.git
cd clip-peft-adaptation

# Create virtual environment and install dependencies
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## **Usage**

### **1. Zero-Shot CLIP**

```bash
python main.py --mode zero-shot --dataset flowers102
```

### **2. CoCoOp**

```bash
python main.py --mode cocoop --shots 16
```

### **3. CLIP-LoRA**

```bash
python main.py --mode lora --shots 16 --r 2 --alpha 32
```

### **4. DISEF**

```bash
python main.py --mode disef --shots 16 --synthetic_data path/to/generated/images
```

---

## **Results**

* **Zero-Shot CLIP**: Baseline accuracy on base and novel classes.
* **CoCoOp**: Improved adaptation with dynamic prompts.
* **CLIP-LoRA**: Parameter-efficient fine-tuning.
* **DISEF**: Performance boost via synthetic augmentation.
* **Improved DISEF**: Further accuracy improvements and cost reduction.

Detailed results with Base, Novel, and Harmonic Mean metrics are reported in the **results/** folder.

---

## **References**

\[1] Radford et al., *Learning Transferable Visual Models From Natural Language Supervision*, ICML 2021
\[2] Zhou et al., *Conditional Prompt Learning for Vision-Language Models*, CVPR 2022
\[3] Hu et al., *LoRA: Low-Rank Adaptation of Large Language Models*, ICLR 2022
\[4] Turrisi et al., *DISEF: Data Injection for Semantic Feature Augmentation*, 2024

---

ðŸ“Œ **Next Steps**

* Extend evaluation to other datasets (e.g., CIFAR-FS, ImageNet-Subset).
* Explore other PEFT techniques (e.g., Adapter Tuning, Prompt Tuning combined with LoRA).

